# ğŸ¤– GPT Chat Local

Uma aplicaÃ§Ã£o de chat local moderna com IA, construÃ­da com Next.js 15, Tailwind CSS e Ollama. Interface elegante com modo escuro/claro, streaming de respostas em tempo real e suporte a mÃºltiplos modelos de linguagem.

## ğŸŒŸ Funcionalidades

- âœ¨ Interface moderna e responsiva
- ğŸ”„ Streaming de respostas em tempo real (SSE)
- ğŸŒ“ Tema claro/escuro automÃ¡tico
- ğŸ’¬ MÃºltiplos chats organizados
- ğŸ“± Totalmente responsivo
- ğŸ¨ Syntax highlighting para cÃ³digo
- ğŸ“‚ Gerenciamento de conversas (criar, arquivar, deletar)
- ğŸš€ Performance otimizada
- ğŸ” Command Palette (Ctrl/Cmd + K)

## ğŸ“‹ PrÃ©-requisitos

Antes de comeÃ§ar, vocÃª precisa ter instalado:

### Para todos os sistemas (Windows, macOS, Linux)

1. **Node.js** (versÃ£o 18 ou superior)
   - [Download Node.js](https://nodejs.org/)
   - Verifique: `node --version` e `npm --version`

2. **Ollama** (para executar modelos de IA localmente)
   - [Download Ollama](https://ollama.ai/)

## ğŸš€ InstalaÃ§Ã£o RÃ¡pida

### 1. Clone o repositÃ³rio
```bash
git clone <url-do-repositorio>
cd gpt
```

### 2. Instale as dependÃªncias
```bash
npm install
```

### 3. Configure as variÃ¡veis de ambiente
```bash
# Copie o arquivo de exemplo
cp .env.example .env

# O arquivo .env jÃ¡ vem configurado para SQLite local
# DATABASE_URL="file:./dev.db"
# OLLAMA_API_URL="http://localhost:11434"
```

### 4. Configure o banco de dados
```bash
# Gere o cliente Prisma
npx prisma generate

# Execute as migraÃ§Ãµes para criar as tabelas
npx prisma migrate dev --name init
```

### 5. Instale e configure o Ollama

#### Windows
1. Baixe o instalador do [site oficial](https://ollama.ai/)
2. Execute o instalador
3. Abra o terminal/PowerShell e execute:

```bash
# Baixe um modelo (exemplo: Llama 3)
ollama pull llama3:latest

# Ou um modelo menor para testes
ollama pull llama3:8b
```

#### macOS
```bash
# Usando Homebrew
brew install ollama

# Ou baixe do site oficial
# https://ollama.ai/

# Inicie o serviÃ§o
ollama serve

# Em outro terminal, baixe um modelo
ollama pull llama3:latest
```

#### Linux
```bash
# InstalaÃ§Ã£o via curl
curl -fsSL https://ollama.ai/install.sh | sh

# Inicie o serviÃ§o
ollama serve

# Em outro terminal, baixe um modelo
ollama pull llama3:latest
```

### 6. Inicie a aplicaÃ§Ã£o
```bash
npm run dev
```

A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em: **http://localhost:3000**

## ğŸ“¦ Modelos DisponÃ­veis

VocÃª pode baixar diferentes modelos com o Ollama:

```bash
# Modelos recomendados
ollama pull llama3:latest      # Modelo principal (4.7GB)
ollama pull llama3:8b          # VersÃ£o menor (4.7GB)
ollama pull codellama          # Especializado em cÃ³digo (3.8GB)
ollama pull mistral            # Modelo alternativo (4.1GB)

# Listar modelos instalados
ollama list
```

## ğŸ› ï¸ Scripts DisponÃ­veis

```bash
npm run dev          # Inicia em modo desenvolvimento
npm run build        # Build para produÃ§Ã£o
npm run start        # Inicia em produÃ§Ã£o
npm run lint         # Executa linting
```

### Scripts do Prisma

```bash
npx prisma studio           # Interface visual do banco
npx prisma migrate dev      # Cria nova migraÃ§Ã£o
npx prisma generate         # Regenera o cliente
npx prisma reset           # Reset completo do banco
```

## ğŸ—„ï¸ Estrutura do Banco

O projeto usa SQLite para simplicidade em desenvolvimento local. As tabelas sÃ£o:

- **Chat**: Armazena as conversas
- **Message**: Armazena as mensagens de cada chat

## ğŸ¨ PersonalizaÃ§Ã£o

### Temas
- A aplicaÃ§Ã£o suporta modo claro/escuro automÃ¡tico
- Baseado nas preferÃªncias do sistema
- AlternÃ¢ncia manual disponÃ­vel

### Modelos de IA
- Configure diferentes modelos no Ollama
- Troca de modelo em tempo real na interface
- Suporte a modelos customizados

## ğŸ”§ Troubleshooting

### Problema: Ollama nÃ£o conecta
```bash
# Verifique se o Ollama estÃ¡ rodando
ollama list

# Se nÃ£o estiver, inicie o serviÃ§o
ollama serve
```

### Problema: Erro de banco de dados
```bash
# Reset o banco e migrations
npx prisma migrate reset

# Regenere o cliente
npx prisma generate
```

### Problema: Porta 3000 ocupada
```bash
# A aplicaÃ§Ã£o tentarÃ¡ usar a porta 3001 automaticamente
# Ou defina uma porta especÃ­fica
PORT=3002 npm run dev
```

### Problema: Node.js muito antigo
```bash
# Verifique a versÃ£o
node --version

# Atualize para Node 18+ se necessÃ¡rio
```

## ğŸš€ Deploy em ProduÃ§Ã£o

### 1. Build da aplicaÃ§Ã£o
```bash
npm run build
```

### 2. ConfiguraÃ§Ã£o do banco
Para produÃ§Ã£o, considere PostgreSQL:

```bash
# .env.production
DATABASE_URL="postgresql://user:password@host:port/database"
```

### 3. ConfiguraÃ§Ã£o do Ollama
Para produÃ§Ã£o, configure o Ollama em um servidor dedicado:

```bash
# .env.production
OLLAMA_API_URL="https://seu-ollama-server.com"
```

## ğŸ“± Recursos da Interface

### Command Palette
- Pressione `Ctrl+K` (Windows/Linux) ou `Cmd+K` (macOS)
- Acesso rÃ¡pido a todas as funÃ§Ãµes

### Atalhos de Teclado
- `Ctrl/Cmd + K`: Abrir command palette
- `Escape`: Fechar chat atual
- `Enter`: Enviar mensagem

### Funcionalidades do Chat
- **Criar**: Novo chat automaticamente
- **Arquivar**: Organizar conversas antigas
- **Deletar**: Remover chats permanentemente
- **Fechar**: Sair da conversa atual

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT.

## ğŸ†˜ Suporte

Se encontrar problemas:

1. Verifique se todas as dependÃªncias estÃ£o instaladas
2. Confirme que o Ollama estÃ¡ rodando
3. Verifique os logs no console do navegador
4. Consulte a documentaÃ§Ã£o do [Ollama](https://ollama.ai/)
5. Abra uma issue no repositÃ³rio

---

**Desenvolvido com â¤ï¸ usando Next.js 15, Tailwind CSS e Ollama**
